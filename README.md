<<<<<<< HEAD
# Analysis of Digital Immigration Discourse on Reddit using NLP and LLM Techniques

**MSc Advanced Computer Science Project | University of Leeds | 2024/2025**

---

## 📜 Project Overview

This repository contains the full source code, analysis pipeline, and interactive dashboard for the MSc dissertation titled "Analysis of Digital Immigration Discourse on Reddit using NLP and LLM Techniques."

The project investigates the public discourse surrounding the UK's move to a digital-first immigration system. By applying a robust Natural Language Processing (NLP) pipeline to a corpus of 1,098 posts from Reddit, this research identifies the key themes of discussion, quantifies the emotional responses of users, and links this public sentiment directly to UK legislation. The primary goal is to provide a data-driven account of the lived experiences of individuals interacting with these new digital systems, offering actionable insights for policymakers and service designers.

### ✨ Key Features

* **End-to-End Data Pipeline:** A reproducible pipeline for scraping, cleaning, and processing data from Reddit and UK government websites.
* **Contextual Topic Modeling:** Utilises BERTopic to discover nuanced, semantically coherent topics from noisy social media text.
* **Fine-Grained Emotion Detection:** Employs the GoEmotions model to classify posts into 27 distinct emotional categories, providing deep affective insights.
* **Topic-Legislation Mapping:** A novel feature that programmatically links emergent topics of public concern to specific UK laws and policies.
* **Interactive Dashboard:** A web-based application built with Streamlit for the dynamic exploration and visualisation of all research findings.

---

## 📁 Project Structure

This repository follows a standard data science project structure to ensure clarity, modularity, and reproducibility.
```
MSc_Project_Digital_Discourse/
│
├── .gitignore          # Specifies files and folders for Git to ignore
├── README.md           # This file, the project's main documentation
├── requirements.txt    # A list of all Python libraries needed to run the project
│
├── final_datasets/     # Contains the final, processed datasets for reference
│   ├── reddit_dashboard_data.csv
│   └── laws_dashboard_data.csv
│
├── data/               # Directory for data generated by the scripts (ignored by Git)
│   ├── raw/
│   └── processed/
│
├── reports/            # For storing generated figures and reports
│   └── figures/
│
├── src/                # Contains all the modular source code for the pipeline
│   ├── init.py
│   ├── collect_data.py
│   ├── process_data.py
│   ├── train_models.py
│   └── analyze_results.py
│
└── app.py              # The main script to launch the Streamlit dashboard
```
---

## 🛠️ Setup and Installation

To run this project, you will need **Python 3.10** or higher and **Git** installed on your system. Follow these steps to set up the environment and install all necessary dependencies.

### 1. Clone the Repository

First, clone this repository to your local machine using Git:

```bash
git clone git clone https://github.com/your-username/MSc_Project_Digital_Discourse.git
cd MSc_Project_Digital_Discourse
```
### 2. Create a Virtual Environment
It is highly recommended to use a virtual environment to manage project dependencies and avoid conflicts with other Python projects.

**On Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**On macOS/Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```
You should now see `(venv)` at the beginning of your command prompt line.

### 3. Install Required Libraries
Install all the required Python libraries from the `requirements.txt` file using pip:

```bash
pip install -r requirements.txt

```
This command will automatically download and install all the necessary packages, such as pandas, praw, streamlit, bertopic, transformers, and their dependencies.

### 4. NLTK Data Download
The project uses the NLTK library for text processing. The first time you run the preprocessing script, it will automatically download the required `punkt` and `stopwords` packages. You can also do this manually:

```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
```
You should now see (venv) at the beginning of your command prompt line.

## 🚀 Usage and Reproducibility
This project is designed to be fully reproducible. The following scripts in the `src/` directory must be run in sequence to execute the entire data pipeline from data collection to analysis.

### Step 1: Collect Raw Data
Scrape the latest data from Reddit and UK government websites.

```bash
python src/collect_data.py
```
**Output:** `data/raw/reddit_scraped_posts.csv`, `data/raw/uk_legislation.csv`

---

### Step 2: Process and Prepare Data

Clean the raw data, apply final topic labels, and merge into dashboard-ready datasets.


```bash
python src/process_data.py
```
**Input:**  
- `data/raw/reddit_scraped_posts.csv`  
- `data/processed/reddit_with_topics.csv`  
- `data/processed/reddit_with_emotions.csv`  

**Output:**  
- `data/processed/reddit_cleaned.csv`  
- `data/processed/reddit_with_final_topics.csv`  
- `data/processed/reddit_dashboard_data.csv`

---

### Step 3: Train Models and Run Matching

Run topic modeling, emotion detection, and topic-legislation mapping.
```bash
python src/train_models.py
```
**Input:** `data/processed/reddit_cleaned.csv`  
**Output:**  
- `data/processed/reddit_with_topics.csv`  
- `data/processed/reddit_with_emotions.csv`  
- `data/processed/topic_legislation_mapping.csv`

---

### Step 4: Generate Report Figures and Summaries

Generate all visualisations (plots, heatmaps) and summary tables.
```bash
python src/analyze_results.py
```

**Input:** `data/processed/reddit_dashboard_data.csv`  
**Output:** Figures saved in `reports/figures/`

---

### Step 5: Launch the Interactive Dashboard

Start the Streamlit web application.
```bash
streamlit run app.py
```
This will start a local web server and open the interactive dashboard in your default browser. 📊

---

## 📀 Reference Datasets

The `/final_datasets` folder includes preprocessed datasets used in the dashboard:

* **reddit_dashboard_data.csv** – The dataset of 1,098 Reddit posts with final topic and emotion labels.  
* **laws_dashboard_data.csv** – The mapping of topics to relevant UK legislation.  

⚠️ While these files are provided for review, the repository ensures full reproducibility. To regenerate datasets, follow the usage instructions above.


---

## 📧 Contact

**Author:** Kartavya Sonar  
**Institution:** University of Leeds  

For any questions regarding this project, please open an issue in this repository.
=======
# digital-immigration-project
>>>>>>> cddb55d7a7adbdca85a30b80a611cde5eb8f32ab
